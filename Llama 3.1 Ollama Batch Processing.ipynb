{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import re\n",
    "import unicodedata\n",
    "from langdetect import detect_langs\n",
    "import ollama\n",
    "\n",
    "\n",
    "class NLPOrchestrator:\n",
    "    def __init__(self, model_id=\"llama3.1:8b-instruct-q4_K_M\"):\n",
    "        print(f\"Using Ollama model: {model_id}\")\n",
    "        self.model_id = model_id\n",
    "\n",
    "        # Language mapping\n",
    "        self.lang_map = {\n",
    "            \"en\": \"English\",\n",
    "            \"hi\": \"Hindi\",\n",
    "            \"te\": \"Telugu\",\n",
    "            \"ta\": \"Tamil\",\n",
    "            \"bn\": \"Bengali\",\n",
    "            \"mr\": \"Marathi\",\n",
    "            \"gu\": \"Gujarati\",\n",
    "            \"kn\": \"Kannada\",\n",
    "            \"ml\": \"Malayalam\",\n",
    "            \"pa\": \"Punjabi\",\n",
    "            \"ur\": \"Urdu\",\n",
    "            \"or\": \"Odia\",\n",
    "            \"as\": \"Assamese\",\n",
    "            \"sa\": \"Sanskrit\",\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Core generation (deterministic)\n",
    "    # -----------------------------\n",
    "    def _generate(self, system_prompt, user_input, max_tokens=512):\n",
    "        response = ollama.chat(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.0,     # critical for JSON stability\n",
    "                \"num_predict\": max_tokens,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        text = response[\"message\"][\"content\"].strip()\n",
    "\n",
    "        gc.collect()\n",
    "        return text\n",
    "\n",
    "    # -----------------------------\n",
    "    # Text cleaning\n",
    "    # -----------------------------\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = text.replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\")\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "        return text.strip()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Romanized Hindi detector\n",
    "    # -----------------------------\n",
    "    def is_romanized_hindi(self, text: str) -> bool:\n",
    "        if not text or len(text) < 20:\n",
    "            return False\n",
    "\n",
    "        ascii_ratio = sum(1 for c in text if ord(c) < 128) / max(len(text), 1)\n",
    "        if ascii_ratio < 0.85:\n",
    "            return False\n",
    "\n",
    "        hinglish_markers = [\n",
    "            \"hai\", \"haan\", \"han\", \"nahi\", \"nahin\", \"kyu\", \"kyun\", \"kya\", \"kaise\",\n",
    "            \"mera\", \"meri\", \"mere\", \"tum\", \"aap\", \"ap\", \"hum\", \"ham\",\n",
    "            \"mat\", \"kar\", \"karo\", \"kariye\", \"krdo\", \"krna\", \"kr diya\",\n",
    "            \"wala\", \"wali\", \"wale\", \"se\", \"ko\", \"me\", \"mein\", \"par\",\n",
    "            \"bahut\", \"bohot\", \"thoda\", \"jaldi\", \"abhi\", \"kal\", \"aaj\"\n",
    "        ]\n",
    "\n",
    "        text_l = \" \" + re.sub(r\"[^a-zA-Z ]\", \" \", text.lower()) + \" \"\n",
    "        hits = sum(1 for w in hinglish_markers if f\" {w} \" in text_l)\n",
    "\n",
    "        return hits >= 2\n",
    "\n",
    "    # -----------------------------\n",
    "    # Language detection\n",
    "    # -----------------------------\n",
    "    def detect_language(self, text):\n",
    "        try:\n",
    "            if self.is_romanized_hindi(text):\n",
    "                return {\n",
    "                    \"lang_list\": [\"Hindi (Romanized)\"],\n",
    "                    \"primary_lang\": \"Hindi (Romanized)\",\n",
    "                }\n",
    "\n",
    "            langs = detect_langs(text)\n",
    "            primary = langs[0].lang\n",
    "            lang_list = [self.lang_map.get(l.lang, l.lang.upper()) for l in langs[:3]]\n",
    "            primary_lang = self.lang_map.get(primary, primary.upper())\n",
    "\n",
    "            return {\"lang_list\": lang_list, \"primary_lang\": primary_lang}\n",
    "\n",
    "        except Exception:\n",
    "            return {\"lang_list\": [\"Unknown\"], \"primary_lang\": \"Unknown\"}\n",
    "\n",
    "    # -----------------------------\n",
    "    # JSON parsing helpers\n",
    "    # -----------------------------\n",
    "    def _try_load_json(self, json_str: str):\n",
    "        if not json_str:\n",
    "            return None\n",
    "\n",
    "        json_str = json_str.strip()\n",
    "        json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
    "        json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
    "        json_str = json_str.replace(\"тАЬ\", \"\\\"\").replace(\"тАЭ\", \"\\\"\").replace(\"тАЩ\", \"'\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    def _parse_json(self, response: str):\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(1))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        match = re.search(r\"```\\s*([\\s\\S]*?)\\s*```\", response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(1))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(0))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        print(\"тЪая╕П JSON parsing failed\")\n",
    "        return None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 1: Translation\n",
    "    # -----------------------------\n",
    "    def step_1_translate(self, text, lang_info):\n",
    "        primary = lang_info[\"primary_lang\"].lower()\n",
    "\n",
    "        if primary == \"english\":\n",
    "            return {\"translated_english_text\": text, \"translation_confidence\": 1.0}\n",
    "\n",
    "        system_prompt = \"\"\"You are a professional translator.\n",
    "Translate the given text into fluent English.\n",
    "\n",
    "Rules:\n",
    "- Translate completely\n",
    "- Preserve meaning and proper nouns\n",
    "- Return ONLY valid JSON\n",
    "\n",
    "Output JSON format:\n",
    "{\n",
    "  \"translated_english_text\": \"...\",\n",
    "  \"translation_confidence\": 0.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "        response = self._generate(system_prompt, text, max_tokens=800)\n",
    "        result = self._parse_json(response)\n",
    "\n",
    "        if result is None:\n",
    "            return {\"translated_english_text\": response, \"translation_confidence\": 0.6}\n",
    "\n",
    "        return result\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 2: Deep analysis\n",
    "    # -----------------------------\n",
    "    def step_2_deep_analysis(self, english_text):\n",
    "        system_prompt = \"\"\"You are a security-focused NLP analyzer.\n",
    "Perform comprehensive analysis on the given text.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Country_iden: Select ONLY ONE value based on content context\n",
    "2. Domains: Use exact capitalization from the list\n",
    "3. Location NER: Only specific geographic places\n",
    "4. Event dates: dd/mm/yyyy ONLY if explicitly mentioned\n",
    "5. Sentiment \"Anti-National\": Only for direct threats to India\n",
    "\n",
    "DOMAIN OPTIONS:\n",
    "Politics, Crime, Military, Terrorism, Radicalisation, Extremism in J&K,\n",
    "Law and Order, Narcotics, Left Wing Extremism, General\n",
    "\n",
    "Return ONLY valid JSON in this format:\n",
    "{\n",
    "  \"domain_ident\": [],\n",
    "  \"sentiment\": \"\",\n",
    "  \"NER\": {\n",
    "    \"Person\": [],\n",
    "    \"Location\": [],\n",
    "    \"Organisation\": [],\n",
    "    \"Event\": [],\n",
    "    \"Product\": []\n",
    "  },\n",
    "  \"Event_calendar\": [],\n",
    "  \"Country_iden\": \"\",\n",
    "  \"Fact_checker\": {\n",
    "    \"relevant_topics\": [],\n",
    "    \"confidence_level\": 0.0,\n",
    "    \"relevance_rating\": \"\"\n",
    "  },\n",
    "  \"Summary\": \"\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "        response = self._generate(system_prompt, english_text, max_tokens=1000)\n",
    "        result = self._parse_json(response)\n",
    "\n",
    "        if result is None:\n",
    "            return {\n",
    "                \"domain_ident\": [\"General\"],\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"NER\": {\n",
    "                    \"Person\": [],\n",
    "                    \"Location\": [],\n",
    "                    \"Organisation\": [],\n",
    "                    \"Event\": [],\n",
    "                    \"Product\": [],\n",
    "                },\n",
    "                \"Event_calendar\": [],\n",
    "                \"Country_iden\": \"Abroad\",\n",
    "                \"Fact_checker\": {\n",
    "                    \"relevant_topics\": [],\n",
    "                    \"confidence_level\": 0.0,\n",
    "                    \"relevance_rating\": \"Low\",\n",
    "                },\n",
    "                \"Summary\": \"Analysis failed.\",\n",
    "            }\n",
    "\n",
    "        return result\n",
    "\n",
    "    # -----------------------------\n",
    "    # Orchestration\n",
    "    # -----------------------------\n",
    "    def process(self, text):\n",
    "    cleaned = self.clean_text(text)\n",
    "\n",
    "    lang_info = self.detect_language(cleaned)\n",
    "    translation = self.step_1_translate(cleaned, lang_info)\n",
    "    analysis = self.step_2_deep_analysis(\n",
    "        translation[\"translated_english_text\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"Cleaned_content\": cleaned,\n",
    "        \"lang_list\": lang_info[\"lang_list\"],\n",
    "        \"primary_lang\": lang_info[\"primary_lang\"],\n",
    "        \"translated_english_text\": translation[\"translated_english_text\"],\n",
    "        **analysis,\n",
    "    }\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "    \"\"\"\n",
    "    texts: List[str]\n",
    "    returns: List[Dict]\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for text in texts:\n",
    "        try:\n",
    "            output = self.process(text)\n",
    "        except Exception as e:\n",
    "            output = {\n",
    "                \"error\": str(e),\n",
    "                \"raw_input\": text\n",
    "            }\n",
    "        results.append(output)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204f0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "content='''роХроЯроирпНрод рокродро┐ройрпИроирпНродрпБ роиро╛роЯрпНроХро│ро╛роХ роЙро▓роХро┐ро▓рпН роОройрпНрой роироЯроирпНродродрпБ?\n",
    "\n",
    "роЗроирпНрод рооро╛род родрпКроЯроХрпНроХродрпНродро┐ро▓рпН ро╡рпЖройро┐роЪрпБро╡рпЗро▓ро╛ро╡ро┐ро▓рпН роорпЗро▒рпНроХрпКро│рпНро│рокрпНрокроЯрпНроЯ ро╡рпЖро▒рпНро▒ро┐роХро░рооро╛рой ро░ро╛рогрпБро╡ роироЯро╡роЯро┐роХрпНроХрпИропро╛ро▓рпН роЙро▒рпНроЪро╛роХроороЯрпИроирпНрод роЯрпКройро╛ро▓рпНроЯрпН роЯро┐ро░роорпНрокрпН , роХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ ро╡ро┐ро╡роХро╛ро░родрпНродро┐ро▓рпН родройродрпБ роЖроХрпНро░рпЛро╖рооро╛рой рокрпЗроЪрпНроЪрпИродрпН родрпКроЯроЩрпНроХро┐ройро╛ро░рпН.\n",
    "\n",
    "роХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ роорпАродро╛рой роЙро░ро┐роорпИ роХрпЛро░ро▓рпНроХро│рпН, ро░ро╛рогрпБро╡ роироЯро╡роЯро┐роХрпНроХрпИ роХрпБро▒ро┐родрпНрод роОроЪрпНроЪро░ро┐роХрпНроХрпИроХро│рпН рооро▒рпНро▒рпБроорпН роРро░рпЛрокрпНрокро╛ро╡ро┐ройрпН рокро╛ро░роорпНрокро░ро┐роп роироЯрпНрокрпБ роиро╛роЯрпБроХро│рпБроХрпНроХрпБ роОродро┐ро░ро╛рой ро╡ро░рпНродрпНродроХ ро╡ро░ро┐роХро│рпН роОрой роЙро▓роХроорпН родро┐ройроорпБроорпН роТро░рпБ роЪрпЖропрпНродро┐ропрпИ роОродро┐ро░рпНроХрпКрогрпНроЯродрпБ.\n",
    "\n",
    "роЖройро╛ро▓рпН роЗрокрпНрокрпЛродрпБ, роЗро╡рпИ роЕройрпИродрпНродрпБроорпН роТро░рпБ рокрпБроХрпИропрпИрокрпН рокрпЛро▓ рооро▒рпИроирпНродрпБро╡ро┐роЯрпНроЯродро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.\n",
    "\n",
    "роЯро┐ро░роорпНрокрпИ роХрпИропро╛ро│рпНро╡родро┐ро▓рпН ро╡ро▓рпНро▓ро╡ро░рпН роОройрпНро▒рпБ роХро░рпБродрокрпНрокроЯрпБроорпН роирпЗроЯрпНроЯрпЛ рокрпКродрпБроЪрпН роЪрпЖропро▓ро╛ро│ро░рпН рооро╛ро░рпНроХрпН ро░рпБроЯрпНроЯрпЗ, роЕродро┐рокро░ро┐ройрпН роЗроирпНрод роЖрокродрпНродро╛рой рокрпЛроХрпНроХрпИроХрпН роХроЯрпНроЯрпБрокрпНрокроЯрпБродрпНродро┐ропродро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.\n",
    "\n",
    "роХроЯроирпНрод ро╡ро╛ро░роорпН роЯрпЖройрпНрооро╛ро░рпНроХрпН рооро▒рпНро▒рпБроорпН роХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ ро╡рпЖро│ро┐ропрпБро▒ро╡рпБ роЕроорпИроЪрпНроЪро░рпНроХро│рпН роЕроорпЖро░ро┐роХрпНроХро╛ро╡рпБроХрпНроХрпБ роорпЗро▒рпНроХрпКрогрпНроЯ рокропрогродрпНродро┐ройрпН рокрпЛродрпБ роЗродро▒рпНроХро╛рой роЕроЯро┐родрпНродро│роорпН роЕроорпИроХрпНроХрокрпНрокроЯрпНроЯро┐ро░рпБроХрпНроХро▓ро╛роорпН.\n",
    "\n",
    "роЕроирпНродрокрпН рокропрогродрпНродро┐ройрпН роорпБроЯро┐ро╡ро┐ро▓рпН, роХро┐ро░рпАройрпНро▓ро╛роирпНродро┐ройрпН роОродро┐ро░рпНроХро╛ро▓роорпН роХрпБро▒ро┐родрпНродрпБ ро╡ро┐ро╡ро╛родро┐роХрпНроХ роТро░рпБ \"роЪрпЖропро▒рпНроХрпБро┤рпБ\" роЕроорпИроХрпНроХ роТрокрпНрокрпБроХрпНроХрпКро│рпНро│рокрпНрокроЯрпНроЯродрпБ.\n",
    "\n",
    "ро╡роЯроХрпНроХрпБ роЕроЯрпНро▓ро╛рогрпНроЯро┐роХрпН роХрпВроЯрпНроЯрогро┐ропрпИропрпЗ роЪро┐родрпИроХрпНроХроХрпНроХрпВроЯро┐роп роТро░рпБ роЪро┐роХрпНроХро▓рпИ ро░рпВроЯрпНроЯрпЗ рооро┐роХ роирпБроЯрпНрокрооро╛роХ роХрпИропро╛рогрпНроЯрпБро│рпНро│родро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a87bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: llama3.1:8b-instruct-q4_K_M\n",
      "============================================================\n",
      "NLP ORCHESTRATION PIPELINE\n",
      "============================================================\n",
      "ЁЯФН Detecting language...\n",
      "ЁЯУЭ Translation...\n",
      "тЪая╕П JSON parsing failed\n",
      "ЁЯФм Deep analysis...\n",
      "\n",
      "============================================================\n",
      "FINAL OUTPUT\n",
      "============================================================\n",
      "{\n",
      "  \"Cleaned_content\": \"роХроЯроирпНрод рокродро┐ройрпИроирпНродрпБ роиро╛роЯрпНроХро│ро╛роХ роЙро▓роХро┐ро▓рпН роОройрпНрой роироЯроирпНродродрпБ?\\n\\nроЗроирпНрод рооро╛род родрпКроЯроХрпНроХродрпНродро┐ро▓рпН ро╡рпЖройро┐роЪрпБро╡рпЗро▓ро╛ро╡ро┐ро▓рпН роорпЗро▒рпНроХрпКро│рпНро│рокрпНрокроЯрпНроЯ ро╡рпЖро▒рпНро▒ро┐роХро░рооро╛рой ро░ро╛рогрпБро╡ роироЯро╡роЯро┐роХрпНроХрпИропро╛ро▓рпН роЙро▒рпНроЪро╛роХроороЯрпИроирпНрод роЯрпКройро╛ро▓рпНроЯрпН роЯро┐ро░роорпНрокрпН , роХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ ро╡ро┐ро╡роХро╛ро░родрпНродро┐ро▓рпН родройродрпБ роЖроХрпНро░рпЛро╖рооро╛рой рокрпЗроЪрпНроЪрпИродрпН родрпКроЯроЩрпНроХро┐ройро╛ро░рпН.\\n\\nроХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ роорпАродро╛рой роЙро░ро┐роорпИ роХрпЛро░ро▓рпНроХро│рпН, ро░ро╛рогрпБро╡ роироЯро╡роЯро┐роХрпНроХрпИ роХрпБро▒ро┐родрпНрод роОроЪрпНроЪро░ро┐роХрпНроХрпИроХро│рпН рооро▒рпНро▒рпБроорпН роРро░рпЛрокрпНрокро╛ро╡ро┐ройрпН рокро╛ро░роорпНрокро░ро┐роп роироЯрпНрокрпБ роиро╛роЯрпБроХро│рпБроХрпНроХрпБ роОродро┐ро░ро╛рой ро╡ро░рпНродрпНродроХ ро╡ро░ро┐роХро│рпН роОрой роЙро▓роХроорпН родро┐ройроорпБроорпН роТро░рпБ роЪрпЖропрпНродро┐ропрпИ роОродро┐ро░рпНроХрпКрогрпНроЯродрпБ.\\n\\nроЖройро╛ро▓рпН роЗрокрпНрокрпЛродрпБ, роЗро╡рпИ роЕройрпИродрпНродрпБроорпН роТро░рпБ рокрпБроХрпИропрпИрокрпН рокрпЛро▓ рооро▒рпИроирпНродрпБро╡ро┐роЯрпНроЯродро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.\\n\\nроЯро┐ро░роорпНрокрпИ роХрпИропро╛ро│рпНро╡родро┐ро▓рпН ро╡ро▓рпНро▓ро╡ро░рпН роОройрпНро▒рпБ роХро░рпБродрокрпНрокроЯрпБроорпН роирпЗроЯрпНроЯрпЛ рокрпКродрпБроЪрпН роЪрпЖропро▓ро╛ро│ро░рпН рооро╛ро░рпНроХрпН ро░рпБроЯрпНроЯрпЗ, роЕродро┐рокро░ро┐ройрпН роЗроирпНрод роЖрокродрпНродро╛рой рокрпЛроХрпНроХрпИроХрпН роХроЯрпНроЯрпБрокрпНрокроЯрпБродрпНродро┐ропродро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.\\n\\nроХроЯроирпНрод ро╡ро╛ро░роорпН роЯрпЖройрпНрооро╛ро░рпНроХрпН рооро▒рпНро▒рпБроорпН роХро┐ро░рпАройрпНро▓ро╛роирпНродрпБ ро╡рпЖро│ро┐ропрпБро▒ро╡рпБ роЕроорпИроЪрпНроЪро░рпНроХро│рпН роЕроорпЖро░ро┐роХрпНроХро╛ро╡рпБроХрпНроХрпБ роорпЗро▒рпНроХрпКрогрпНроЯ рокропрогродрпНродро┐ройрпН рокрпЛродрпБ роЗродро▒рпНроХро╛рой роЕроЯро┐родрпНродро│роорпН роЕроорпИроХрпНроХрокрпНрокроЯрпНроЯро┐ро░рпБроХрпНроХро▓ро╛роорпН.\\n\\nроЕроирпНродрокрпН рокропрогродрпНродро┐ройрпН роорпБроЯро┐ро╡ро┐ро▓рпН, роХро┐ро░рпАройрпНро▓ро╛роирпНродро┐ройрпН роОродро┐ро░рпНроХро╛ро▓роорпН роХрпБро▒ро┐родрпНродрпБ ро╡ро┐ро╡ро╛родро┐роХрпНроХ роТро░рпБ \\\"роЪрпЖропро▒рпНроХрпБро┤рпБ\\\" роЕроорпИроХрпНроХ роТрокрпНрокрпБроХрпНроХрпКро│рпНро│рокрпНрокроЯрпНроЯродрпБ.\\n\\nро╡роЯроХрпНроХрпБ роЕроЯрпНро▓ро╛рогрпНроЯро┐роХрпН роХрпВроЯрпНроЯрогро┐ропрпИропрпЗ роЪро┐родрпИроХрпНроХроХрпНроХрпВроЯро┐роп роТро░рпБ роЪро┐роХрпНроХро▓рпИ ро░рпВроЯрпНроЯрпЗ рооро┐роХ роирпБроЯрпНрокрооро╛роХ роХрпИропро╛рогрпНроЯрпБро│рпНро│родро╛роХродрпН родрпЖро░ро┐роХро┐ро▒родрпБ.\",\n",
      "  \"lang_list\": [\n",
      "    \"Tamil\"\n",
      "  ],\n",
      "  \"primary_lang\": \"Tamil\",\n",
      "  \"translated_english_text\": \"{\\n  \\\"translated_english_text\\\": \\\"What has been happening in the world for the past 15 days?\\n\\nUS President Donald Trump, who was thrilled by the successful military operation carried out in Venezuela at the beginning of this month, began his fiery speech on the Greenland issue.\\n\\nThe demands for sovereignty over Greenland, warnings about military action and trade tariffs against Europe's traditional friendly countries are what the world has been facing every day.\\n\\nBut now, all these seem to have disappeared like a cloud.\\n\\nNATO General Secretary Mark Rutte, who is known as an expert in handling Trump, seems to have brought his president's dangerous trend under control.\\n\\nLast week, Danish and Greenland foreign ministers may have laid the groundwork for this during their trip to America.\\n\\nAt the end of that trip, it was agreed to set up a 'working group' to discuss the future of Greenland.\\n\\nRutte has handled this complex issue with great finesse, which could potentially break up NATO.\\\",\\n  \\\"translation_confidence\\\": 0.0\\n}\",\n",
      "  \"domain_ident\": [\n",
      "    \"Politics\"\n",
      "  ],\n",
      "  \"sentiment\": \"Neutral\",\n",
      "  \"NER\": {\n",
      "    \"Person\": [\n",
      "      \"Donald Trump\",\n",
      "      \"Mark Rutte\"\n",
      "    ],\n",
      "    \"Location\": [\n",
      "      \"Venezuela\",\n",
      "      \"Greenland\",\n",
      "      \"Europe\",\n",
      "      \"America\"\n",
      "    ],\n",
      "    \"Organisation\": [\n",
      "      \"NATO\"\n",
      "    ],\n",
      "    \"Event\": [],\n",
      "    \"Product\": []\n",
      "  },\n",
      "  \"Event_calendar\": [],\n",
      "  \"Country_iden\": \"United States\",\n",
      "  \"Fact_checker\": {\n",
      "    \"relevant_topics\": [\n",
      "      \"US President Donald Trump's speech on Greenland issue\",\n",
      "      \"NATO General Secretary Mark Rutte's handling of the situation\"\n",
      "    ],\n",
      "    \"confidence_level\": 0.8,\n",
      "    \"relevance_rating\": \"High\"\n",
      "  },\n",
      "  \"Summary\": \"The article discusses US President Donald Trump's recent speech on the Greenland issue and how NATO General Secretary Mark Rutte has handled the situation, potentially averting a crisis.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    orchestrator = NLPOrchestrator()\n",
    "\n",
    "    test_inputs = [\n",
    "        \"Mujhe aaj bhi yaad hai woh din, 2 April 2011...\",\n",
    "        \"India conducted a military exercise near the border.\",\n",
    "        \"The suspect was arrested by Delhi Police on Monday.\"\n",
    "    ]\n",
    "\n",
    "    results = orchestrator.process_batch(test_inputs)\n",
    "\n",
    "    print(json.dumps(results, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a1a45e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
