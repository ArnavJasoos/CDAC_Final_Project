{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86052e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import re\n",
    "import unicodedata\n",
    "from langdetect import detect_langs\n",
    "import ollama\n",
    "\n",
    "\n",
    "class NLPOrchestrator:\n",
    "    def __init__(self, model_id=\"qwen3-vl:8b-instruct\"):\n",
    "        print(f\"Using Ollama model: {model_id}\")\n",
    "        self.model_id = model_id\n",
    "\n",
    "        # Language mapping\n",
    "        self.lang_map = {\n",
    "            \"en\": \"English\",\n",
    "            \"hi\": \"Hindi\",\n",
    "            \"te\": \"Telugu\",\n",
    "            \"ta\": \"Tamil\",\n",
    "            \"bn\": \"Bengali\",\n",
    "            \"mr\": \"Marathi\",\n",
    "            \"gu\": \"Gujarati\",\n",
    "            \"kn\": \"Kannada\",\n",
    "            \"ml\": \"Malayalam\",\n",
    "            \"pa\": \"Punjabi\",\n",
    "            \"ur\": \"Urdu\",\n",
    "            \"or\": \"Odia\",\n",
    "            \"as\": \"Assamese\",\n",
    "            \"sa\": \"Sanskrit\",\n",
    "        }\n",
    "\n",
    "    # -----------------------------\n",
    "    # Core generation (deterministic)\n",
    "    # -----------------------------\n",
    "    def _generate(self, system_prompt, user_input, max_tokens=512):\n",
    "        response = ollama.chat(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.0,     # critical for JSON stability\n",
    "                \"num_predict\": max_tokens,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        text = response[\"message\"][\"content\"].strip()\n",
    "\n",
    "        gc.collect()\n",
    "        return text\n",
    "\n",
    "    # -----------------------------\n",
    "    # Text cleaning\n",
    "    # -----------------------------\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = text.replace(\"\\u200c\", \"\").replace(\"\\u200d\", \"\")\n",
    "        text = text.replace(\"\\ufeff\", \"\")\n",
    "        text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "        text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)\n",
    "        return text.strip()\n",
    "\n",
    "    # -----------------------------\n",
    "    # Romanized Hindi detector\n",
    "    # -----------------------------\n",
    "    def is_romanized_hindi(self, text: str) -> bool:\n",
    "        if not text or len(text) < 20:\n",
    "            return False\n",
    "\n",
    "        ascii_ratio = sum(1 for c in text if ord(c) < 128) / max(len(text), 1)\n",
    "        if ascii_ratio < 0.85:\n",
    "            return False\n",
    "\n",
    "        hinglish_markers = [\n",
    "            \"hai\", \"haan\", \"han\", \"nahi\", \"nahin\", \"kyu\", \"kyun\", \"kya\", \"kaise\",\n",
    "            \"mera\", \"meri\", \"mere\", \"tum\", \"aap\", \"ap\", \"hum\", \"ham\",\n",
    "            \"mat\", \"kar\", \"karo\", \"kariye\", \"krdo\", \"krna\", \"kr diya\",\n",
    "            \"wala\", \"wali\", \"wale\", \"se\", \"ko\", \"me\", \"mein\", \"par\",\n",
    "            \"bahut\", \"bohot\", \"thoda\", \"jaldi\", \"abhi\", \"kal\", \"aaj\"\n",
    "        ]\n",
    "\n",
    "        text_l = \" \" + re.sub(r\"[^a-zA-Z ]\", \" \", text.lower()) + \" \"\n",
    "        hits = sum(1 for w in hinglish_markers if f\" {w} \" in text_l)\n",
    "\n",
    "        return hits >= 2\n",
    "\n",
    "    # -----------------------------\n",
    "    # Language detection\n",
    "    # -----------------------------\n",
    "    def detect_language(self, text):\n",
    "        try:\n",
    "            if self.is_romanized_hindi(text):\n",
    "                return {\n",
    "                    \"lang_list\": [\"Hindi (Romanized)\"],\n",
    "                    \"primary_lang\": \"Hindi (Romanized)\",\n",
    "                }\n",
    "\n",
    "            langs = detect_langs(text)\n",
    "            primary = langs[0].lang\n",
    "            lang_list = [self.lang_map.get(l.lang, l.lang.upper()) for l in langs[:3]]\n",
    "            primary_lang = self.lang_map.get(primary, primary.upper())\n",
    "\n",
    "            return {\"lang_list\": lang_list, \"primary_lang\": primary_lang}\n",
    "\n",
    "        except Exception:\n",
    "            return {\"lang_list\": [\"Unknown\"], \"primary_lang\": \"Unknown\"}\n",
    "\n",
    "    # -----------------------------\n",
    "    # JSON parsing helpers\n",
    "    # -----------------------------\n",
    "    def _try_load_json(self, json_str: str):\n",
    "        if not json_str:\n",
    "            return None\n",
    "\n",
    "        json_str = json_str.strip()\n",
    "        json_str = re.sub(r\",\\s*}\", \"}\", json_str)\n",
    "        json_str = re.sub(r\",\\s*]\", \"]\", json_str)\n",
    "        json_str = json_str.replace(\"‚Äú\", \"\\\"\").replace(\"‚Äù\", \"\\\"\").replace(\"‚Äô\", \"'\")\n",
    "\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "\n",
    "    def _parse_json(self, response: str):\n",
    "        if not response:\n",
    "            return None\n",
    "\n",
    "        match = re.search(r\"```json\\s*([\\s\\S]*?)\\s*```\", response, re.IGNORECASE)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(1))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        match = re.search(r\"```\\s*([\\s\\S]*?)\\s*```\", response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(1))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        match = re.search(r\"\\{[\\s\\S]*\\}\", response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(0))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "\n",
    "        print(\"‚ö†Ô∏è JSON parsing failed\")\n",
    "        return None\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 1: Translation\n",
    "    # -----------------------------\n",
    "    def step_1_translate(self, text, lang_info):\n",
    "        primary = lang_info[\"primary_lang\"].lower()\n",
    "\n",
    "        if primary == \"english\":\n",
    "            return {\"translated_english_text\": text, \"translation_confidence\": 1.0}\n",
    "\n",
    "        system_prompt = \"\"\"You are a professional translator.\n",
    "Translate the given text into fluent English.\n",
    "\n",
    "Rules:\n",
    "- Translate completely\n",
    "- Preserve meaning and proper nouns\n",
    "- Return ONLY valid JSON\n",
    "\n",
    "Output JSON format:\n",
    "{\n",
    "  \"translated_english_text\": \"...\",\n",
    "  \"translation_confidence\": 0.0\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "        response = self._generate(system_prompt, text, max_tokens=800)\n",
    "        result = self._parse_json(response)\n",
    "\n",
    "        if result is None:\n",
    "            return {\"translated_english_text\": response, \"translation_confidence\": 0.6}\n",
    "\n",
    "        return result\n",
    "\n",
    "    # -----------------------------\n",
    "    # Step 2: Deep analysis\n",
    "    # -----------------------------\n",
    "    def step_2_deep_analysis(self, english_text):\n",
    "        system_prompt = \"\"\"You are a security-focused NLP analyzer.\n",
    "Perform comprehensive analysis on the given text.\n",
    "\n",
    "CRITICAL RULES:\n",
    "1. Country_iden: Select ONLY ONE value based on content context\n",
    "2. Domains: Use exact capitalization from the list\n",
    "3. Location NER: Only specific geographic places\n",
    "4. Event dates: dd/mm/yyyy ONLY if explicitly mentioned\n",
    "5. Sentiment \"Anti-National\": Only for direct threats to India\n",
    "\n",
    "DOMAIN OPTIONS:\n",
    "Politics, Crime, Military, Terrorism, Radicalisation, Extremism in J&K,\n",
    "Law and Order, Narcotics, Left Wing Extremism, General\n",
    "\n",
    "Return ONLY valid JSON in this format:\n",
    "{\n",
    "  \"domain_ident\": [],\n",
    "  \"sentiment\": \"\",\n",
    "  \"NER\": {\n",
    "    \"Person\": [],\n",
    "    \"Location\": [],\n",
    "    \"Organisation\": [],\n",
    "    \"Event\": [],\n",
    "    \"Product\": []\n",
    "  },\n",
    "  \"Event_calendar\": [],\n",
    "  \"Country_iden\": \"\",\n",
    "  \"Fact_checker\": {\n",
    "    \"relevant_topics\": [],\n",
    "    \"confidence_level\": 0.0,\n",
    "    \"relevance_rating\": \"\"\n",
    "  },\n",
    "  \"Summary\": \"\"\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "        response = self._generate(system_prompt, english_text, max_tokens=1000)\n",
    "        result = self._parse_json(response)\n",
    "\n",
    "        if result is None:\n",
    "            return {\n",
    "                \"domain_ident\": [\"General\"],\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"NER\": {\n",
    "                    \"Person\": [],\n",
    "                    \"Location\": [],\n",
    "                    \"Organisation\": [],\n",
    "                    \"Event\": [],\n",
    "                    \"Product\": [],\n",
    "                },\n",
    "                \"Event_calendar\": [],\n",
    "                \"Country_iden\": \"Abroad\",\n",
    "                \"Fact_checker\": {\n",
    "                    \"relevant_topics\": [],\n",
    "                    \"confidence_level\": 0.0,\n",
    "                    \"relevance_rating\": \"Low\",\n",
    "                },\n",
    "                \"Summary\": \"Analysis failed.\",\n",
    "            }\n",
    "\n",
    "        return result\n",
    "\n",
    "    # -----------------------------\n",
    "    # Orchestration\n",
    "    # -----------------------------\n",
    "    def process(self, text):\n",
    "        cleaned = self.clean_text(text)\n",
    "\n",
    "        print(\"üîç Detecting language...\")\n",
    "        lang_info = self.detect_language(cleaned)\n",
    "\n",
    "        print(\"üìù Translation...\")\n",
    "        translation = self.step_1_translate(cleaned, lang_info)\n",
    "\n",
    "        print(\"üî¨ Deep analysis...\")\n",
    "        analysis = self.step_2_deep_analysis(translation[\"translated_english_text\"])\n",
    "\n",
    "        return {\n",
    "            \"Cleaned_content\": cleaned,\n",
    "            \"lang_list\": lang_info[\"lang_list\"],\n",
    "            \"primary_lang\": lang_info[\"primary_lang\"],\n",
    "            \"translated_english_text\": translation[\"translated_english_text\"],\n",
    "            **analysis,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f3eee76",
   "metadata": {},
   "outputs": [],
   "source": [
    "content='''‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡•Ä ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§°‡•ã‡§®‡§æ‡§≤‡•ç‡§° ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ó‡•Å‡§∞‡•Å‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§ï‡§•‡§ø‡§§ '‡§¨‡•ã‡§∞‡•ç‡§° ‡§ë‡§´ ‡§™‡•Ä‡§∏' ‡§™‡§π‡§≤ ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡§∞ ‡§¶‡•Ä.\n",
    "\n",
    "‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§á‡§∏‡§ï‡§æ ‡§Æ‡§ï‡§º‡§∏‡§¶ ‡§ó‡§º‡§ú‡§º‡§æ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∞‡§æ‡§á‡§≤ ‡§î‡§∞ ‡§π‡§Æ‡§æ‡§∏ ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§ï‡•ã ‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä ‡§¨‡§®‡§æ‡§®‡§æ ‡§î‡§∞ ‡§´‡§º‡§≤‡§∏‡•ç‡§§‡•Ä‡§®‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Ö‡§Ç‡§§‡§∞‡§ø‡§Æ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§®‡§ø‡§ó‡§∞‡§æ‡§®‡•Ä ‡§ï‡§∞‡§®‡§æ ‡§π‡•à.\n",
    "\n",
    "‡§≠‡§æ‡§∞‡§§ ‡§â‡§® ‡§¶‡§∞‡•ç‡§ú‡§®‡•ã‡§Ç ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à, ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§á‡§∏ ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡§æ ‡§®‡§ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡§£ ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à.\n",
    "\n",
    "‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø ‡§Ö‡§≠‡•Ä ‡§§‡§ï ‡§Ø‡§π ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§ï‡§ø ‡§≠‡§æ‡§∞‡§§ ‡§á‡§∏‡•á ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡•á‡§ó‡§æ ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç. ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∞‡§æ‡§á‡§≤ ‡§®‡•á ‡§≠‡•Ä ‡§á‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§Ö‡§∏‡§π‡§Æ‡§§‡§ø ‡§ú‡§§‡§æ‡§à ‡§•‡•Ä ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞ ‡§≤‡§ø‡§Ø‡§æ ‡§•‡§æ.\n",
    "\n",
    "‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡•Ä ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§°‡•ã‡§®‡§æ‡§≤‡•ç‡§° ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ó‡•Å‡§∞‡•Å‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§ë‡§´ ‡§™‡•Ä‡§∏ ‡§ï‡•Ä ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡•Ä ‡§§‡•ã ‡§≠‡§æ‡§∞‡§§ ‡§â‡§® ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§•‡§æ ‡§ú‡•ã ‡§á‡§∏ ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§Æ‡•á‡§Ç ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§•‡•á.\n",
    "\n",
    "‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§â‡§® ‡§ï‡§à ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§®‡•á‡§§‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§•‡•á, ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§Æ‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à.\n",
    "\n",
    "‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§ï‡•á ‡§®‡§ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡§£ ‡§ï‡•ã ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®, ‡§§‡•Å‡§∞‡•ç‡§ï‡•Ä, ‡§∏‡§ä‡§¶‡•Ä ‡§Ö‡§∞‡§¨ ‡§î‡§∞ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§Ö‡§∞‡§¨ ‡§Ö‡§Æ‡•Ä‡§∞‡§æ‡§§ ‡§≠‡•Ä ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç.\n",
    "\n",
    "‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø 59 ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§∏‡•ç‡§§‡§æ‡§ï‡•ç‡§∑‡§∞ ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§¶‡§æ‡§µ‡•ã‡§∏ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§Æ‡§Ç‡§ö ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡•á‡§µ‡§≤ 19 ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø ‡§π‡•Ä ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§•‡•á.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be6a725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Ollama model: qwen3-vl:8b-instruct\n",
      "============================================================\n",
      "NLP ORCHESTRATION PIPELINE\n",
      "============================================================\n",
      "üîç Detecting language...\n",
      "üìù Translation...\n",
      "üî¨ Deep analysis...\n",
      "\n",
      "============================================================\n",
      "FINAL OUTPUT\n",
      "============================================================\n",
      "{\n",
      "  \"Cleaned_content\": \"‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡•Ä ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§°‡•ã‡§®‡§æ‡§≤‡•ç‡§° ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ó‡•Å‡§∞‡•Å‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§ï‡§•‡§ø‡§§ '‡§¨‡•ã‡§∞‡•ç‡§° ‡§ë‡§´ ‡§™‡•Ä‡§∏' ‡§™‡§π‡§≤ ‡§ï‡•Ä ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡§∞ ‡§¶‡•Ä.\\n\\n‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§ï‡§æ ‡§ï‡§π‡§®‡§æ ‡§π‡•à ‡§ï‡§ø ‡§á‡§∏‡§ï‡§æ ‡§Æ‡§ï‡§º‡§∏‡§¶ ‡§ó‡§º‡§ú‡§º‡§æ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∞‡§æ‡§á‡§≤ ‡§î‡§∞ ‡§π‡§Æ‡§æ‡§∏ ‡§ï‡•á ‡§¨‡•Ä‡§ö ‡§∏‡§Ç‡§ò‡§∞‡•ç‡§∑‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§ï‡•ã ‡§∏‡•ç‡§•‡§æ‡§Ø‡•Ä ‡§¨‡§®‡§æ‡§®‡§æ ‡§î‡§∞ ‡§´‡§º‡§≤‡§∏‡•ç‡§§‡•Ä‡§®‡•Ä ‡§ï‡•ç‡§∑‡•á‡§§‡•ç‡§∞ ‡§Æ‡•á‡§Ç ‡§è‡§ï ‡§Ö‡§Ç‡§§‡§∞‡§ø‡§Æ ‡§∏‡§∞‡§ï‡§æ‡§∞ ‡§ï‡•Ä ‡§®‡§ø‡§ó‡§∞‡§æ‡§®‡•Ä ‡§ï‡§∞‡§®‡§æ ‡§π‡•à.\\n\\n‡§≠‡§æ‡§∞‡§§ ‡§â‡§® ‡§¶‡§∞‡•ç‡§ú‡§®‡•ã‡§Ç ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à, ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§á‡§∏ ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡§æ ‡§®‡§ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡§£ ‡§Æ‡§ø‡§≤‡§æ ‡§π‡•à.\\n\\n‡§π‡§æ‡§≤‡§æ‡§Ç‡§ï‡§ø ‡§Ö‡§≠‡•Ä ‡§§‡§ï ‡§Ø‡§π ‡§∏‡•ç‡§™‡§∑‡•ç‡§ü ‡§®‡§π‡•Ä‡§Ç ‡§π‡•à ‡§ï‡§ø ‡§≠‡§æ‡§∞‡§§ ‡§á‡§∏‡•á ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡•á‡§ó‡§æ ‡§Ø‡§æ ‡§®‡§π‡•Ä‡§Ç. ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§Æ‡•á‡§Ç ‡§á‡§∏‡§∞‡§æ‡§á‡§≤ ‡§®‡•á ‡§≠‡•Ä ‡§á‡§∏‡•á ‡§≤‡•á‡§ï‡§∞ ‡§Ö‡§∏‡§π‡§Æ‡§§‡§ø ‡§ú‡§§‡§æ‡§à ‡§•‡•Ä ‡§≤‡•á‡§ï‡§ø‡§® ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞ ‡§≤‡§ø‡§Ø‡§æ ‡§•‡§æ.\\n\\n‡§Ö‡§Æ‡•á‡§∞‡§ø‡§ï‡•Ä ‡§∞‡§æ‡§∑‡•ç‡§ü‡•ç‡§∞‡§™‡§§‡§ø ‡§°‡•ã‡§®‡§æ‡§≤‡•ç‡§° ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ó‡•Å‡§∞‡•Å‡§µ‡§æ‡§∞ ‡§ï‡•ã ‡§Ö‡§™‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§ë‡§´ ‡§™‡•Ä‡§∏ ‡§ï‡•Ä ‡§î‡§™‡§ö‡§æ‡§∞‡§ø‡§ï ‡§∂‡•Å‡§∞‡•Å‡§Ü‡§§ ‡§ï‡•Ä ‡§§‡•ã ‡§≠‡§æ‡§∞‡§§ ‡§â‡§® ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§•‡§æ ‡§ú‡•ã ‡§á‡§∏ ‡§∏‡§Æ‡§æ‡§∞‡•ã‡§π ‡§Æ‡•á‡§Ç ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§®‡§π‡•Ä‡§Ç ‡§•‡•á.\\n\\n‡§™‡•ç‡§∞‡§ß‡§æ‡§®‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§®‡§∞‡•á‡§Ç‡§¶‡•ç‡§∞ ‡§Æ‡•ã‡§¶‡•Ä ‡§â‡§® ‡§ï‡§à ‡§µ‡•à‡§∂‡•ç‡§µ‡§ø‡§ï ‡§®‡•á‡§§‡§æ‡§ì‡§Ç ‡§Æ‡•á‡§Ç ‡§•‡•á, ‡§ú‡§ø‡§®‡•ç‡§π‡•á‡§Ç ‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§Ü‡§Æ‡§Ç‡§§‡•ç‡§∞‡§ø‡§§ ‡§ï‡§ø‡§Ø‡§æ ‡§π‡•à.\\n\\n‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§ï‡•á ‡§®‡§ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡§£ ‡§ï‡•ã ‡§∏‡•ç‡§µ‡•Ä‡§ï‡§æ‡§∞ ‡§ï‡§∞‡§®‡•á ‡§µ‡§æ‡§≤‡•á ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§Æ‡•á‡§Ç ‡§™‡§æ‡§ï‡§ø‡§∏‡•ç‡§§‡§æ‡§®, ‡§§‡•Å‡§∞‡•ç‡§ï‡•Ä, ‡§∏‡§ä‡§¶‡•Ä ‡§Ö‡§∞‡§¨ ‡§î‡§∞ ‡§∏‡§Ç‡§Ø‡•Å‡§ï‡•ç‡§§ ‡§Ö‡§∞‡§¨ ‡§Ö‡§Æ‡•Ä‡§∞‡§æ‡§§ ‡§≠‡•Ä ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•à‡§Ç.\\n\\n‡§ü‡•ç‡§∞‡§Ç‡§™ ‡§®‡•á ‡§ï‡§π‡§æ ‡§ï‡§ø 59 ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§®‡•á ‡§¨‡•ã‡§∞‡•ç‡§° ‡§Æ‡•á‡§Ç ‡§∂‡§æ‡§Æ‡§ø‡§≤ ‡§π‡•ã‡§®‡•á ‡§ï‡•á ‡§≤‡§ø‡§è ‡§π‡§∏‡•ç‡§§‡§æ‡§ï‡•ç‡§∑‡§∞ ‡§ï‡§ø‡§è ‡§π‡•à‡§Ç ‡§≤‡•á‡§ï‡§ø‡§® ‡§¶‡§æ‡§µ‡•ã‡§∏ ‡§Æ‡•á‡§Ç ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§Ü‡§∞‡•ç‡§•‡§ø‡§ï ‡§Æ‡§Ç‡§ö ‡§ï‡•á ‡§¶‡•å‡§∞‡§æ‡§® ‡§Ü‡§Ø‡•ã‡§ú‡§ø‡§§ ‡§á‡§∏ ‡§ï‡§æ‡§∞‡•ç‡§Ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§Ç ‡§ï‡•á‡§µ‡§≤ 19 ‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡•á ‡§™‡•ç‡§∞‡§§‡§ø‡§®‡§ø‡§ß‡§ø ‡§π‡•Ä ‡§Æ‡•å‡§ú‡•Ç‡§¶ ‡§•‡•á.\",\n",
      "  \"lang_list\": [\n",
      "    \"Hindi\"\n",
      "  ],\n",
      "  \"primary_lang\": \"Hindi\",\n",
      "  \"translated_english_text\": \"On Thursday, U.S. President Donald Trump launched his so-called 'Board of Peace' initiative. Trump says the goal is to make the ceasefire between Israel and Hamas in Gaza permanent and to oversee a temporary government in the Palestinian territories. India is among the dozens of countries invited to join this board. However, it is still unclear whether India will accept it. Initially, Israel also objected to the initiative but later accepted it. When Trump formally launched his Board of Peace on Thursday, India was among the countries not present at the event. Prime Minister Narendra Modi was among the many global leaders whom Trump invited to join the board. Countries that accepted Trump's invitation include Pakistan, Turkey, Saudi Arabia, and the United Arab Emirates. Trump said that 59 countries have signed up to join the board, but only 19 countries' representatives were present at the event, which was held during the World Economic Forum in Davos.\",\n",
      "  \"domain_ident\": [\n",
      "    \"Politics\"\n",
      "  ],\n",
      "  \"sentiment\": \"\",\n",
      "  \"NER\": {\n",
      "    \"Person\": [\n",
      "      \"Donald Trump\",\n",
      "      \"Narendra Modi\"\n",
      "    ],\n",
      "    \"Location\": [\n",
      "      \"United States\",\n",
      "      \"Israel\",\n",
      "      \"Hamas\",\n",
      "      \"Gaza\",\n",
      "      \"Palestinian territories\",\n",
      "      \"Davos\"\n",
      "    ],\n",
      "    \"Organisation\": [\n",
      "      \"Board of Peace\",\n",
      "      \"World Economic Forum\"\n",
      "    ],\n",
      "    \"Event\": [],\n",
      "    \"Product\": []\n",
      "  },\n",
      "  \"Event_calendar\": [],\n",
      "  \"Country_iden\": \"United States\",\n",
      "  \"Fact_checker\": {\n",
      "    \"relevant_topics\": [\n",
      "      \"U.S. foreign policy\",\n",
      "      \"Middle East peace process\",\n",
      "      \"India's diplomatic engagement\",\n",
      "      \"World Economic Forum\"\n",
      "    ],\n",
      "    \"confidence_level\": 0.95,\n",
      "    \"relevance_rating\": \"High\"\n",
      "  },\n",
      "  \"Summary\": \"U.S. President Donald Trump launched the 'Board of Peace' initiative aimed at securing a permanent ceasefire between Israel and Hamas and establishing a temporary Palestinian government. India was invited but not present at the event in Davos, while countries like Pakistan, Turkey, Saudi Arabia, and UAE accepted. Trump claimed 59 countries signed up, but only 19 attended.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    extractor = NLPOrchestrator()\n",
    "\n",
    "    # Test text (Bengali)\n",
    "    text = content\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"NLP ORCHESTRATION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    result = extractor.process(text)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ba247",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
