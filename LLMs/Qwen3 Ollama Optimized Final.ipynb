{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c595c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gc\n",
    "import re\n",
    "import unicodedata\n",
    "import hashlib\n",
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from langdetect import detect_langs\n",
    "import ollama\n",
    "\n",
    "\n",
    "class MultiLanguageRomanizationDetector:\n",
    "    \"\"\"Efficient multi-language romanization detector\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Language-specific marker words (most common romanized words)\n",
    "        self.markers = {\n",
    "            \"hindi\": {\n",
    "                \"hai\", \"haan\", \"nahi\", \"nahin\", \"kyu\", \"kya\", \"kaise\",\n",
    "                \"mera\", \"meri\", \"mere\", \"tera\", \"teri\", \"tum\", \"aap\",\n",
    "                \"kar\", \"karo\", \"karna\", \"kiya\", \"hum\", \"main\",\n",
    "                \"bahut\", \"thoda\", \"jaldi\", \"abhi\", \"kal\", \"aaj\",\n",
    "                \"wala\", \"wali\", \"wale\", \"se\", \"ko\", \"me\", \"mein\",\n",
    "                \"achha\", \"bura\", \"bhi\", \"baat\", \"sahi\", \"galat\"\n",
    "            },\n",
    "            \"tamil\": {\n",
    "                \"antha\", \"ithu\", \"athu\", \"enna\", \"epdi\", \"eppadi\",\n",
    "                \"naan\", \"nee\", \"avan\", \"aval\", \"namma\", \"ungal\",\n",
    "                \"illa\", \"irukku\", \"irukkadhu\", \"panna\", \"pannu\",\n",
    "                \"nalla\", \"ketta\", \"romba\", \"konjam\", \"ippo\", \"naalaiku\",\n",
    "                \"da\", \"di\", \"nga\", \"ma\", \"pa\", \"sollu\", \"solla\",\n",
    "                \"vaa\", \"po\", \"poda\", \"vara\", \"poga\", \"vandha\"\n",
    "            },\n",
    "            \"telugu\": {\n",
    "                \"andi\", \"enti\", \"ela\", \"ela unnaru\", \"chala\",\n",
    "                \"nenu\", \"nuvvu\", \"vaadu\", \"aame\", \"manam\", \"meeru\",\n",
    "                \"ledu\", \"undi\", \"undadu\", \"cheyyi\", \"chey\",\n",
    "                \"manchidi\", \"cheddadi\", \"chala\", \"konchem\", \"ippudu\", \"repu\",\n",
    "                \"ra\", \"ri\", \"ru\", \"anna\", \"akka\", \"cheppu\", \"cheppandi\",\n",
    "                \"raa\", \"vellu\", \"vellandi\", \"vacchi\", \"velli\"\n",
    "            },\n",
    "            \"bengali\": {\n",
    "                \"aache\", \"nai\", \"nei\", \"keno\", \"ki\", \"kivabe\",\n",
    "                \"amar\", \"tomar\", \"tumi\", \"apni\", \"amra\", \"ora\",\n",
    "                \"koro\", \"korben\", \"korechi\", \"hobe\", \"hoyeche\",\n",
    "                \"bhalo\", \"kharap\", \"onek\", \"ektu\", \"ekhon\", \"kal\",\n",
    "                \"bolo\", \"bolben\", \"jao\", \"jan\", \"esho\", \"elen\"\n",
    "            },\n",
    "            \"marathi\": {\n",
    "                \"aahe\", \"nahi\", \"kay\", \"kasa\", \"kuthe\",\n",
    "                \"maza\", \"tuja\", \"tu\", \"tumhi\", \"aamhi\", \"tyanche\",\n",
    "                \"kar\", \"kara\", \"kela\", \"hota\", \"honar\",\n",
    "                \"changle\", \"vait\", \"khup\", \"thode\", \"aata\", \"udya\",\n",
    "                \"sang\", \"sanga\", \"ja\", \"ya\", \"ye\", \"aala\"\n",
    "            },\n",
    "            \"gujarati\": {\n",
    "                \"chhe\", \"nathi\", \"kai\", \"kem\", \"kya\",\n",
    "                \"maru\", \"taru\", \"tame\", \"aapne\", \"ame\", \"teo\",\n",
    "                \"karo\", \"karjo\", \"karyu\", \"hashe\", \"thayu\",\n",
    "                \"saru\", \"kharaab\", \"ghanu\", \"thodu\", \"aaje\", \"kale\",\n",
    "                \"bolo\", \"kahjo\", \"jao\", \"aavo\", \"avyo\"\n",
    "            },\n",
    "            \"kannada\": {\n",
    "                \"ide\", \"illa\", \"enu\", \"hege\", \"elli\",\n",
    "                \"nanna\", \"ninna\", \"neevu\", \"avanu\", \"avalu\", \"naavu\",\n",
    "                \"maadi\", \"maadu\", \"maadiruva\", \"agutte\", \"aayitu\",\n",
    "                \"chennagide\", \"ketta\", \"thumba\", \"swalp╨░\", \"eeega\", \"naale\",\n",
    "                \"heli\", \"he─╝i\", \"hogi\", \"baa\", \"bandaru\"\n",
    "            },\n",
    "            \"malayalam\": {\n",
    "                \"aanu\", \"alla\", \"enthu\", \"engane\", \"evide\",\n",
    "                \"ente\", \"ninte\", \"ningal\", \"avan\", \"aval\", \"nammal\",\n",
    "                \"cheyyuka\", \"cheyyu\", \"cheythu\", \"aakum\", \"aayi\",\n",
    "                \"nalla\", \"cheeththa\", \"valare\", \"koracchu\", \"ippo\", \"naale\",\n",
    "                \"parayuka\", \"paRa\", \"poda\", \"vaa\", \"vannu\"\n",
    "            },\n",
    "            \"punjabi\": {\n",
    "                \"hai\", \"nahi\", \"ki\", \"kive\", \"kithe\",\n",
    "                \"mera\", \"tera\", \"tussi\", \"assi\", \"ohna\",\n",
    "                \"karo\", \"karna\", \"kita\", \"hona\", \"hoya\",\n",
    "                \"changa\", \"mara\", \"bahut\", \"thoda\", \"hun\", \"kal\",\n",
    "                \"dassi\", \"dasso\", \"jao\", \"aao\", \"aaya\"\n",
    "            },\n",
    "            \"urdu\": {\n",
    "                \"hai\", \"nahi\", \"kya\", \"kaise\", \"kahan\",\n",
    "                \"mera\", \"tera\", \"aap\", \"tum\", \"hum\", \"unka\",\n",
    "                \"karo\", \"karna\", \"kiya\", \"hoga\", \"hua\",\n",
    "                \"acha\", \"bura\", \"bahut\", \"thora\", \"abhi\", \"kal\",\n",
    "                \"bolo\", \"kahiye\", \"jao\", \"aayiye\", \"aaya\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Character n-grams specific to languages\n",
    "        self.bigrams = {\n",
    "            \"tamil\": {\"ll\", \"nn\", \"tt\", \"pp\", \"kk\", \"th\", \"ng\", \"la\"},\n",
    "            \"telugu\": {\"nd\", \"nt\", \"mp\", \"mb\", \"ll\", \"nn\", \"tt\"},\n",
    "            \"bengali\": {\"ch\", \"sh\", \"kh\", \"gh\", \"bh\", \"dh\"},\n",
    "            \"kannada\": {\"tt\", \"dd\", \"nn\", \"ll\", \"ge\", \"de\"},\n",
    "            \"malayalam\": {\"kk\", \"tt\", \"pp\", \"ll\", \"nn\", \"th\"},\n",
    "        }\n",
    "        \n",
    "        self.alpha_only_pattern = re.compile(r\"[^a-zA-Z ]\")\n",
    "    \n",
    "    def detect(self, text: str) -> Tuple[Optional[str], float]:\n",
    "        \"\"\"\n",
    "        Detect romanized language with confidence score.\n",
    "        Returns: (language_name, confidence_score)\n",
    "        \"\"\"\n",
    "        if not text or len(text) < 20:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Quick ASCII check\n",
    "        ascii_ratio = sum(1 for c in text if ord(c) < 128) / len(text)\n",
    "        if ascii_ratio < 0.85:\n",
    "            return None, 0.0\n",
    "        \n",
    "        text_l = \" \" + self.alpha_only_pattern.sub(\" \", text.lower()) + \" \"\n",
    "        text_len = len(text.split())\n",
    "        \n",
    "        scores = {}\n",
    "        \n",
    "        # Check marker words for each language\n",
    "        for lang, markers in self.markers.items():\n",
    "            hits = sum(1 for word in markers if f\" {word} \" in text_l)\n",
    "            if hits > 0:\n",
    "                # Confidence based on marker density\n",
    "                confidence = min(hits / max(text_len * 0.15, 1), 1.0)\n",
    "                scores[lang] = confidence\n",
    "        \n",
    "        # Bigram analysis for South Indian languages\n",
    "        bigrams_in_text = {text[i:i+2].lower() for i in range(len(text)-1) \n",
    "                          if text[i:i+2].isalpha()}\n",
    "        \n",
    "        for lang, bigrams in self.bigrams.items():\n",
    "            matches = len(bigrams_in_text & bigrams)\n",
    "            if matches > 0:\n",
    "                bigram_conf = min(matches / 5, 0.4)  # Max 0.4 from bigrams\n",
    "                scores[lang] = scores.get(lang, 0) + bigram_conf\n",
    "        \n",
    "        if not scores:\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Get language with highest score\n",
    "        best_lang = max(scores.items(), key=lambda x: x[1])\n",
    "        \n",
    "        if best_lang[1] >= 0.3:  # Minimum threshold\n",
    "            lang_name = best_lang[0].capitalize()\n",
    "            return f\"{lang_name} (Romanized)\", min(best_lang[1], 0.95)\n",
    "        \n",
    "        return None, 0.0\n",
    "\n",
    "\n",
    "class NLPOrchestrator:\n",
    "    def __init__(self, model_id=\"llama3.1:8b-instruct-q4_K_M\"):\n",
    "        print(f\"Using Ollama model: {model_id}\")\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # Initialize romanization detector\n",
    "        self.romanization_detector = MultiLanguageRomanizationDetector()\n",
    "        \n",
    "        # Language mapping\n",
    "        self.lang_map = {\n",
    "            \"en\": \"English\", \"hi\": \"Hindi\", \"te\": \"Telugu\", \"ta\": \"Tamil\",\n",
    "            \"bn\": \"Bengali\", \"mr\": \"Marathi\", \"gu\": \"Gujarati\", \"kn\": \"Kannada\",\n",
    "            \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"ur\": \"Urdu\", \"or\": \"Odia\",\n",
    "            \"as\": \"Assamese\", \"sa\": \"Sanskrit\",\n",
    "        }\n",
    "        \n",
    "        # Pre-compiled regex patterns\n",
    "        self.unicode_cleanup = str.maketrans({\n",
    "            \"\\u200c\": \"\", \"\\u200d\": \"\", \"\\ufeff\": \"\"\n",
    "        })\n",
    "        self.space_pattern = re.compile(r\"[ \\t]+\")\n",
    "        self.newline_pattern = re.compile(r\"\\n{3,}\")\n",
    "        self.trailing_comma_pattern = re.compile(r\",\\s*([}\\]])\")\n",
    "        self.json_block_pattern = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", re.IGNORECASE)\n",
    "        self.json_object_pattern = re.compile(r\"\\{[\\s\\S]*\\}\")\n",
    "        \n",
    "        # Cache for results\n",
    "        self._cache = {}\n",
    "    \n",
    "    def _get_cache_key(self, text: str, operation: str) -> str:\n",
    "        \"\"\"Generate cache key for text + operation\"\"\"\n",
    "        return f\"{operation}:{hashlib.md5(text.encode()).hexdigest()}\"\n",
    "    \n",
    "    def _generate(self, system_prompt: str, user_input: str, max_tokens: int = 512) -> str:\n",
    "        \"\"\"Core generation method\"\"\"\n",
    "        response = ollama.chat(\n",
    "            model=self.model_id,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_input},\n",
    "            ],\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "                \"num_predict\": max_tokens,\n",
    "            },\n",
    "        )\n",
    "        \n",
    "        text = response[\"message\"][\"content\"].strip()\n",
    "        gc.collect()\n",
    "        return text\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Optimized text cleaning\"\"\"\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "        text = text.translate(self.unicode_cleanup)\n",
    "        text = self.space_pattern.sub(\" \", text)\n",
    "        text = self.newline_pattern.sub(\"\\n\\n\", text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def _clean_json_string(self, json_str: str) -> str:\n",
    "        \"\"\"Clean JSON string\"\"\"\n",
    "        json_str = json_str.strip()\n",
    "        json_str = self.trailing_comma_pattern.sub(r\"\\1\", json_str)\n",
    "        json_str = json_str.replace(\"\"\", \"\\\"\").replace(\"\"\", \"\\\"\").replace(\"'\", \"'\")\n",
    "        return json_str\n",
    "    \n",
    "    def _try_load_json(self, json_str: str) -> Optional[Dict]:\n",
    "        \"\"\"Attempt to parse JSON\"\"\"\n",
    "        if not json_str:\n",
    "            return None\n",
    "        \n",
    "        cleaned = self._clean_json_string(json_str)\n",
    "        try:\n",
    "            return json.loads(cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def _parse_json(self, response: str) -> Optional[Dict]:\n",
    "        \"\"\"Extract and parse JSON from response\"\"\"\n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        # Try code block\n",
    "        match = self.json_block_pattern.search(response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(1))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "        \n",
    "        # Try direct object\n",
    "        match = self.json_object_pattern.search(response)\n",
    "        if match:\n",
    "            parsed = self._try_load_json(match.group(0))\n",
    "            if parsed:\n",
    "                return parsed\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def detect_language(self, text: str) -> Dict:\n",
    "        \"\"\"Enhanced language detection with romanization support\"\"\"\n",
    "        cache_key = self._get_cache_key(text, \"lang_detect\")\n",
    "        if cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        try:\n",
    "            # Check for romanized languages first\n",
    "            romanized_lang, rom_confidence = self.romanization_detector.detect(text)\n",
    "            \n",
    "            if romanized_lang and rom_confidence >= 0.3:\n",
    "                result = {\n",
    "                    \"lang_list\": [romanized_lang],\n",
    "                    \"primary_lang\": romanized_lang,\n",
    "                    \"confidence\": round(rom_confidence, 3),\n",
    "                    \"detection_method\": \"romanization_markers\"\n",
    "                }\n",
    "                self._cache[cache_key] = result\n",
    "                return result\n",
    "            \n",
    "            # Fallback to langdetect\n",
    "            langs = detect_langs(text)\n",
    "            primary = langs[0].lang\n",
    "            lang_list = [self.lang_map.get(l.lang, l.lang.upper()) for l in langs[:3]]\n",
    "            primary_lang = self.lang_map.get(primary, primary.upper())\n",
    "            \n",
    "            result = {\n",
    "                \"lang_list\": lang_list,\n",
    "                \"primary_lang\": primary_lang,\n",
    "                \"confidence\": round(langs[0].prob, 3),\n",
    "                \"detection_method\": \"langdetect\"\n",
    "            }\n",
    "            self._cache[cache_key] = result\n",
    "            return result\n",
    "            \n",
    "        except Exception:\n",
    "            result = {\n",
    "                \"lang_list\": [\"Unknown\"],\n",
    "                \"primary_lang\": \"Unknown\",\n",
    "                \"confidence\": 0.0,\n",
    "                \"detection_method\": \"fallback\"\n",
    "            }\n",
    "            return result\n",
    "    \n",
    "    def _calculate_english_ratio(self, text: str) -> float:\n",
    "        \"\"\"Calculate percentage of English content\"\"\"\n",
    "        try:\n",
    "            sentences = text.split('.')\n",
    "            english_count = 0\n",
    "            \n",
    "            for sent in sentences[:10]:  # Sample first 10 sentences\n",
    "                sent = sent.strip()\n",
    "                if len(sent) < 10:\n",
    "                    continue\n",
    "                try:\n",
    "                    langs = detect_langs(sent)\n",
    "                    if langs and langs[0].lang == 'en' and langs[0].prob > 0.8:\n",
    "                        english_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return english_count / min(len(sentences), 10) if sentences else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def step_1_translate(self, text: str, lang_info: Dict) -> Dict:\n",
    "        \"\"\"Optimized translation with confidence scoring\"\"\"\n",
    "        primary = lang_info[\"primary_lang\"].lower()\n",
    "        \n",
    "        # Skip translation if already English\n",
    "        if \"english\" in primary:\n",
    "            return {\n",
    "                \"translated_text\": text,\n",
    "                \"translation_confidence\": 1.0,\n",
    "                \"translation_method\": \"skipped_native_english\"\n",
    "            }\n",
    "        \n",
    "        # Check if mostly English\n",
    "        english_ratio = self._calculate_english_ratio(text)\n",
    "        if english_ratio > 0.8:\n",
    "            return {\n",
    "                \"translated_text\": text,\n",
    "                \"translation_confidence\": 0.95,\n",
    "                \"translation_method\": \"skipped_high_english_ratio\"\n",
    "            }\n",
    "        \n",
    "        # Compact prompt with \"what not to do\" approach\n",
    "        system_prompt = \"\"\"Translate to English. Return JSON only.\n",
    "\n",
    "DON'T:\n",
    "- Add explanations or preambles\n",
    "- Use markdown formatting\n",
    "- Translate proper nouns (names, places)\n",
    "- Add content not in original\n",
    "- Change meaning or tone\n",
    "\n",
    "Format:\n",
    "{\"translated_text\": \"...\", \"confidence\": 0.0}\"\"\"\n",
    "        \n",
    "        cache_key = self._get_cache_key(text, \"translate\")\n",
    "        if cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        response = self._generate(system_prompt, text, max_tokens=len(text.split()) * 3)\n",
    "        result = self._parse_json(response)\n",
    "        \n",
    "        if result is None:\n",
    "            result = {\n",
    "                \"translated_text\": response,\n",
    "                \"translation_confidence\": 0.5,\n",
    "                \"translation_method\": \"fallback_raw_response\"\n",
    "            }\n",
    "        else:\n",
    "            # Handle nested JSON\n",
    "            if \"translated_text\" in result:\n",
    "                trans_value = result[\"translated_text\"]\n",
    "                if isinstance(trans_value, str) and trans_value.strip().startswith(\"{\"):\n",
    "                    nested = self._try_load_json(trans_value)\n",
    "                    if nested and \"translated_text\" in nested:\n",
    "                        result = nested\n",
    "            \n",
    "            result[\"translation_method\"] = \"llm_translation\"\n",
    "            result[\"translation_confidence\"] = result.get(\"confidence\", 0.85)\n",
    "            if \"confidence\" in result:\n",
    "                del result[\"confidence\"]\n",
    "        \n",
    "        self._cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def step_2_deep_analysis(self, english_text: str) -> Dict:\n",
    "        \"\"\"Optimized deep analysis with confidence scores\"\"\"\n",
    "        # Compact prompt with \"what not to do\" approach\n",
    "        system_prompt = \"\"\"Analyze text. Return JSON only.\n",
    "\n",
    "DON'T:\n",
    "- Invent facts not in text\n",
    "- Use multiple values for Country_iden (pick ONE)\n",
    "- Add Event_calendar dates without explicit mentions (dd/mm/yyyy format required)\n",
    "- Mark sentiment as \"Anti-National\" unless direct threats to India\n",
    "- Use domain names not in list below\n",
    "- Include markdown, preambles, or explanations\n",
    "- Leave confidence scores at 0.0 (estimate realistically)\n",
    "\n",
    "Domains: Politics, Crime, Military, Terrorism, Radicalisation, Extremism in J&K, Law and Order, Narcotics, Left Wing Extremism, General\n",
    "\n",
    "Format:\n",
    "{\n",
    "  \"domain_ident\": [],\n",
    "  \"domain_confidence\": 0.0,\n",
    "  \"sentiment\": \"\",\n",
    "  \"sentiment_confidence\": 0.0,\n",
    "  \"NER\": {\n",
    "    \"Person\": [],\n",
    "    \"Location\": [],\n",
    "    \"Organisation\": [],\n",
    "    \"Event\": [],\n",
    "    \"Product\": []\n",
    "  },\n",
    "  \"ner_confidence\": 0.0,\n",
    "  \"Event_calendar\": [],\n",
    "  \"Country_iden\": \"\",\n",
    "  \"country_confidence\": 0.0,\n",
    "  \"Fact_checker\": {\n",
    "    \"relevant_topics\": [],\n",
    "    \"confidence_level\": 0.0,\n",
    "    \"relevance_rating\": \"\"\n",
    "  },\n",
    "  \"Summary\": \"\"\n",
    "}\"\"\"\n",
    "        \n",
    "        cache_key = self._get_cache_key(english_text, \"analysis\")\n",
    "        if cache_key in self._cache:\n",
    "            return self._cache[cache_key]\n",
    "        \n",
    "        # Dynamic token allocation based on text length\n",
    "        text_tokens = len(english_text.split())\n",
    "        max_tokens = min(400 + (text_tokens // 2), 800)\n",
    "        \n",
    "        response = self._generate(system_prompt, english_text, max_tokens=max_tokens)\n",
    "        result = self._parse_json(response)\n",
    "        \n",
    "        if result is None:\n",
    "            result = {\n",
    "                \"domain_ident\": [\"General\"],\n",
    "                \"domain_confidence\": 0.3,\n",
    "                \"sentiment\": \"Neutral\",\n",
    "                \"sentiment_confidence\": 0.3,\n",
    "                \"NER\": {\n",
    "                    \"Person\": [], \"Location\": [], \"Organisation\": [],\n",
    "                    \"Event\": [], \"Product\": [],\n",
    "                },\n",
    "                \"ner_confidence\": 0.0,\n",
    "                \"Event_calendar\": [],\n",
    "                \"Country_iden\": \"Unknown\",\n",
    "                \"country_confidence\": 0.0,\n",
    "                \"Fact_checker\": {\n",
    "                    \"relevant_topics\": [],\n",
    "                    \"confidence_level\": 0.0,\n",
    "                    \"relevance_rating\": \"Low\",\n",
    "                },\n",
    "                \"Summary\": \"Analysis failed - JSON parsing error.\",\n",
    "            }\n",
    "        \n",
    "        self._cache[cache_key] = result\n",
    "        return result\n",
    "    \n",
    "    def process(self, text: str) -> Dict:\n",
    "        \"\"\"Main orchestration pipeline\"\"\"\n",
    "        cleaned = self.clean_text(text)\n",
    "        \n",
    "        print(\"ЁЯФН Detecting language...\")\n",
    "        lang_info = self.detect_language(cleaned)\n",
    "        \n",
    "        print(\"ЁЯУЭ Translation...\")\n",
    "        translation = self.step_1_translate(cleaned, lang_info)\n",
    "        \n",
    "        print(\"ЁЯФм Deep analysis...\")\n",
    "        analysis = self.step_2_deep_analysis(translation[\"translated_text\"])\n",
    "        \n",
    "        # Compile final output\n",
    "        return {\n",
    "            \"metadata\": {\n",
    "                \"language_detection\": {\n",
    "                    \"primary_lang\": lang_info[\"primary_lang\"],\n",
    "                    \"lang_list\": lang_info[\"lang_list\"],\n",
    "                    \"confidence\": lang_info[\"confidence\"],\n",
    "                    \"method\": lang_info[\"detection_method\"]\n",
    "                },\n",
    "                \"translation\": {\n",
    "                    \"confidence\": translation[\"translation_confidence\"],\n",
    "                    \"method\": translation[\"translation_method\"]\n",
    "                }\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"original\": cleaned,\n",
    "                \"translated\": translation[\"translated_text\"]\n",
    "            },\n",
    "            \"analysis\": {\n",
    "                \"domain\": {\n",
    "                    \"categories\": analysis[\"domain_ident\"],\n",
    "                    \"confidence\": analysis.get(\"domain_confidence\", 0.0)\n",
    "                },\n",
    "                \"sentiment\": {\n",
    "                    \"label\": analysis[\"sentiment\"],\n",
    "                    \"confidence\": analysis.get(\"sentiment_confidence\", 0.0)\n",
    "                },\n",
    "                \"entities\": {\n",
    "                    \"NER\": analysis[\"NER\"],\n",
    "                    \"confidence\": analysis.get(\"ner_confidence\", 0.0)\n",
    "                },\n",
    "                \"geography\": {\n",
    "                    \"country\": analysis[\"Country_iden\"],\n",
    "                    \"confidence\": analysis.get(\"country_confidence\", 0.0)\n",
    "                },\n",
    "                \"events\": analysis[\"Event_calendar\"],\n",
    "                \"fact_check\": analysis[\"Fact_checker\"],\n",
    "                \"summary\": analysis[\"Summary\"]\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    content = '''ркнрк╛рк░ркдрлАркп ркЯрлАркорлЗ ркирлНркпрлВркЭрлАрк▓рлЗркирлНркб рк╕рк╛ркорлЗ рк╕ркдркд ркмрлАркЬрлА T20I ркЬрлАркдрлАркирлЗ рк╢рлНрк░рлЗркгрлАркорк╛ркВ 2-0ркирлА рк▓рлАркб ркорлЗрк│рк╡рлА ркЫрлЗ. рк░рк╛ркпрккрлБрк░ркорк╛ркВ рк░ркорк╛ркпрлЗрк▓рлА ркмрлАркЬрлА ркорлЗркЪркорк╛ркВ, ркЯрлАрко ркИркирлНркбрк┐ркпрк╛ркП ркорк╛ркдрлНрк░ 92 ркмрлЛрк▓ркорк╛ркВ 209 рк░ркиркирлЛ рк▓ркХрлНрк╖рлНркпрк╛ркВркХ рк╣рк╛ркВрк╕рк▓ ркХрк░рлНркпрлЛ. ркЖ ркЬрлАркдркорк╛ркВ ркнрк╛рк░ркдрлАркп ркЯрлАрко ркорк╛ркЯрлЗ ркХрлЗрккрлНркЯрки рк╕рлВрк░рлНркпркХрлБркорк╛рк░ ркпрк╛ркжрк╡рлЗ рк╕рлМркерлА рк╡ркзрлБ рк░рки ркмркирк╛рк╡рлНркпрк╛, 37 ркмрлЛрк▓ркорк╛ркВ 82 рк░рки ркмркирк╛рк╡рлНркпрк╛. ркЬрлЛркХрлЗ, ркЬрлАркдркирлЛ рк╡рк╛рк╕рлНркдрк╡рк┐ркХ рккрк╛ркпрлЛ ркИрк╢рк╛рки ркХрк┐рк╢рки ркжрлНрк╡рк╛рк░рк╛ ркирк╛ркЦрк╡рк╛ркорк╛ркВ ркЖрк╡рлНркпрлЛ рк╣ркдрлЛ, ркдрлЗркгрлЗ ркорлБрк╢рлНркХрлЗрк▓ рккрк░рк┐рк╕рлНркерк┐ркдрк┐ркУркорк╛ркВ ркЬрлЗ рк░рлАркдрлЗ рккрк░рклрлЛрк░рлНрко ркХрк░рлА ркЕркбркзрлА рк╕ркжрлА ркмркирк╛рк╡рлА ркдрлЗ ркЬрлЛркИркирлЗ ркмркзрк╛ ркЬ ркЦрлБрк╢ ркеркИ ркЧркпрк╛. ркЬрлЛркХрлЗ, ркЖ ркЗркирк┐ркВркЧ ркжрк░ркорк┐ркпрк╛рки, рк╕рлВрк░рлНркпркХрлБркорк╛рк░ ркИрк╢рк╛рки рккрк░ ркЧрлБрк╕рлНрк╕рлЗ ркеркИ ркЧркпрк╛ рк╣ркдрк╛, ркЕркирлЗ ркдрлЗркоркгрлЗ ркорлЗркЪ рккркЫрлА ркЖ рк╡рк╛ркдркирлЛ ркЦрлБрк▓рк╛рк╕рлЛ ркХрк░рлНркпрлЛ.'''\n",
    "    \n",
    "    extractor = NLPOrchestrator()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"NLP ORCHESTRATION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = extractor.process(content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
