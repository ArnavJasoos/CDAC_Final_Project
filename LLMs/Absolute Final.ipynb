{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19365f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import ollama\n",
    "from langdetect import detect_langs\n",
    "from collections import OrderedDict  # for simple LRU-like cache\n",
    "\n",
    "\n",
    "class UnicodeScriptDetector:\n",
    "    \n",
    "    SCRIPT_RANGES = {\n",
    "        \"Hindi\": (0x0900, 0x097F),\n",
    "        \"Bengali\": (0x0980, 0x09FF),\n",
    "        \"Punjabi\": (0x0A00, 0x0A7F),\n",
    "        \"Gujarati\": (0x0A80, 0x0AFF),\n",
    "        \"Odia\": (0x0B00, 0x0B7F),\n",
    "        \"Tamil\": (0x0B80, 0x0BFF),\n",
    "        \"Telugu\": (0x0C00, 0x0C7F),\n",
    "        \"Kannada\": (0x0C80, 0x0CFF),\n",
    "        \"Malayalam\": (0x0D00, 0x0D7F),\n",
    "    }\n",
    "    \n",
    "    @staticmethod\n",
    "    def detect(text):\n",
    "        if not text or len(text) < 10:\n",
    "            return None, 0.0\n",
    "        \n",
    "        counts = {lang: 0 for lang in UnicodeScriptDetector.SCRIPT_RANGES}\n",
    "        total = 0\n",
    "        \n",
    "        for char in text:\n",
    "            if not char.isalpha():\n",
    "                continue\n",
    "            total += 1\n",
    "            cp = ord(char)\n",
    "            for lang, (start, end) in UnicodeScriptDetector.SCRIPT_RANGES.items():\n",
    "                if start <= cp <= end:\n",
    "                    counts[lang] += 1\n",
    "                    break\n",
    "        \n",
    "        if total == 0:\n",
    "            return None, 0.0\n",
    "        \n",
    "        best = max(counts.items(), key=lambda x: x[1])\n",
    "        if best[1] > 0:\n",
    "            conf = best[1] / total\n",
    "            if conf >= 0.3:\n",
    "                return best[0], min(conf, 0.99)\n",
    "        \n",
    "        return None, 0.0\n",
    "\n",
    "\n",
    "class RomanizedHindiDetector:\n",
    "    \n",
    "    MARKERS = frozenset({\n",
    "        \"hai\", \"haan\", \"han\", \"nahi\", \"nahin\", \"kyu\", \"kyun\", \"kya\", \"kaise\",\n",
    "        \"mera\", \"meri\", \"mere\", \"tera\", \"teri\", \"tum\", \"aap\", \"ap\",\n",
    "        \"kar\", \"karo\", \"karna\", \"kiya\", \"hum\", \"ham\", \"main\",\n",
    "        \"bahut\", \"bohot\", \"thoda\", \"jaldi\", \"abhi\", \"kal\", \"aaj\",\n",
    "        \"wala\", \"wali\", \"wale\", \"se\", \"ko\", \"me\", \"mein\", \"par\",\n",
    "        \"achha\", \"acha\", \"bura\", \"bhi\", \"baat\", \"sahi\", \"galat\",\n",
    "        \"mat\", \"kariye\", \"krdo\", \"krna\", \"kr\", \"diya\"\n",
    "    })\n",
    "    \n",
    "    ALPHA_RE = re.compile(r\"[^a-zA-Z ]\")\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(cls, text):\n",
    "        if not text or len(text) < 20:\n",
    "            return None, 0.0\n",
    "        \n",
    "        if sum(1 for c in text if ord(c) < 128) / len(text) < 0.85:\n",
    "            return None, 0.0\n",
    "        \n",
    "        words = set(cls.ALPHA_RE.sub(\" \", text.lower()).split())\n",
    "        hits = len(words & cls.MARKERS)\n",
    "        \n",
    "        if hits < 2:\n",
    "            return None, 0.0\n",
    "        \n",
    "        conf = min(hits / max(len(text.split()) * 0.12, 1), 0.95)\n",
    "        return (\"Hindi (Romanized)\", conf) if conf >= 0.25 else (None, 0.0)\n",
    "\n",
    "\n",
    "class MarathiDetector:\n",
    "    \n",
    "    MARKERS = frozenset({\n",
    "        \"आहे\", \"आहेत\", \"नाही\", \"नाहीत\", \"झाला\", \"झाली\", \"झाले\", \"झालेला\", \"झालेली\", \"झालेल्या\",\n",
    "        \"करतो\", \"करते\", \"करतात\", \"केले\", \"केली\", \"केलेला\", \"केलेली\",\n",
    "        \"होता\", \"होती\", \"होते\", \"होत\", \"असतात\", \"असे\", \"असून\",\n",
    "        \"यांनी\", \"यांचा\", \"यांची\", \"यांचे\", \"यांना\", \"यांच्यावर\", \"यांच्याकडून\",\n",
    "        \"चा\", \"ची\", \"चे\", \"च्या\", \"ला\", \"ना\", \"मध्ये\", \"वर\", \"साठी\", \"मुळे\", \"पर्यंत\",\n",
    "        \"कडून\", \"प्रमाणे\", \"संबंधित\", \"प्रस्ताव\", \"अधिवेशन\", \"विधेयक\", \"सभागृह\", \"खासदार\",\n",
    "        \"म्हणजे\", \"म्हणून\", \"काय\", \"का\", \"की\", \"बघा\", \"दिला\", \"दिली\", \"दिले\",\n",
    "        \"गेला\", \"गेली\", \"गेलो\", \"पाठवले\", \"आम्ही\", \"तुम्ही\", \"त्यांनी\", \"त्याचा\"\n",
    "    })\n",
    "    \n",
    "    ALPHA_RE = re.compile(r\"[^ऀ-ॿ ]\")\n",
    "    \n",
    "    @classmethod\n",
    "    def detect(cls, text):\n",
    "        if not text or len(text) < 15:\n",
    "            return None, 0.0\n",
    "        \n",
    "        devanagari_count = sum(1 for c in text if 0x0900 <= ord(c) <= 0x097F)\n",
    "        if devanagari_count / max(len(text), 1) < 0.6:\n",
    "            return None, 0.0\n",
    "        \n",
    "        normalized = text.replace(\"।\", \" \").replace(\",\", \" \").replace(\"!\", \" \").replace(\"?\", \" \")\n",
    "        words = set(cls.ALPHA_RE.sub(\" \", normalized).split())\n",
    "        \n",
    "        hits = len(words & cls.MARKERS)\n",
    "        \n",
    "        if hits < 1:\n",
    "            return None, 0.0\n",
    "        \n",
    "        word_count = max(len(words), 1)\n",
    "        conf = min(hits / word_count * 3.0, 0.95)\n",
    "        return (\"Marathi\", conf) if conf >= 0.25 else (None, 0.0)\n",
    "\n",
    "\n",
    "class NLPOrchestrator:\n",
    "    def __init__(self, model_id=\"llama3.1:8b-instruct-q4_K_M\"):\n",
    "        print(f\"Using Ollama model: {model_id}\")\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        self.cleanup_trans = str.maketrans({\"\\u200c\": \"\", \"\\u200d\": \"\", \"\\ufeff\": \"\"})\n",
    "        self.space_re = re.compile(r\"\\s+\")\n",
    "        self.trailing_comma_re = re.compile(r\",\\s*([}\\]])\")\n",
    "        self.json_block_re = re.compile(r\"```(?:json)?\\s*([\\s\\S]*?)\\s*```\", re.IGNORECASE)\n",
    "        \n",
    "        # Simple cache for LLM calls (prompt prefix + max_tokens)\n",
    "        self._llm_cache = OrderedDict()\n",
    "        self._cache_max_size = 300  # prevent unbounded memory growth\n",
    "    \n",
    "    def callllm(self, prompt, max_tokens=800):\n",
    "        # Cache key: first 400 chars of prompt + max_tokens\n",
    "        cache_key = f\"{max_tokens}:{prompt[:400]}\"\n",
    "        \n",
    "        if cache_key in self._llm_cache:\n",
    "            return self._llm_cache[cache_key]\n",
    "        \n",
    "        resp = ollama.chat(\n",
    "            model=self.model_id,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\n",
    "                \"temperature\": 0.0,\n",
    "                \"num_predict\": max_tokens,\n",
    "                \"top_p\": 0.9   # added for slightly faster/more focused output\n",
    "            },\n",
    "        )\n",
    "        result = resp[\"message\"][\"content\"].strip()\n",
    "        \n",
    "        # Store in cache\n",
    "        self._llm_cache[cache_key] = result\n",
    "        if len(self._llm_cache) > self._cache_max_size:\n",
    "            self._llm_cache.popitem(last=False)  # remove oldest\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def clean(self, text):\n",
    "        text = unicodedata.normalize(\"NFKC\", text).translate(self.cleanup_trans)\n",
    "        return self.space_re.sub(\" \", text).strip()\n",
    "    \n",
    "    def parsejson(self, response):\n",
    "        if not response:\n",
    "            return None\n",
    "        \n",
    "        response = response.replace(\"\"\", '\"').replace(\"\"\", '\"').replace(\"'\", \"'\")\n",
    "        response = self.trailing_comma_re.sub(r\"\\1\", response)\n",
    "        \n",
    "        match = self.json_block_re.search(response)\n",
    "        if match:\n",
    "            try:\n",
    "                return json.loads(match.group(1))\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        start = response.find(\"{\")\n",
    "        end = response.rfind(\"}\") + 1\n",
    "        if start != -1 and end > start:\n",
    "            try:\n",
    "                return json.loads(response[start:end])\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def llmdetectlanguage(self, text):\n",
    "        prompt = f\"\"\"What language is this text written in?\n",
    "Only return the language name in English. Examples:\n",
    "- Hindi\n",
    "- Marathi\n",
    "- Punjabi\n",
    "- English\n",
    "- Bengali\n",
    "\n",
    "Text:\n",
    "{text[:400]}\"\"\"\n",
    "        \n",
    "        resp = self.callllm(prompt, max_tokens=30)\n",
    "        lang = resp.strip()\n",
    "        \n",
    "        lang = lang.replace(\"Hindi.\", \"Hindi\").replace(\"Marathi.\", \"Marathi\")\n",
    "        if \"marathi\" in lang.lower():\n",
    "            lang = \"Marathi\"\n",
    "        elif \"hindi\" in lang.lower():\n",
    "            lang = \"Hindi\"\n",
    "        \n",
    "        return {\"primary_lang\": lang, \"confidence\": 0.92, \"method\": \"llm\"}\n",
    "    \n",
    "    def detectlanguage(self, text):\n",
    "        lang, conf = UnicodeScriptDetector.detect(text)\n",
    "        if lang and conf >= 0.3:\n",
    "            if lang == \"Hindi\":\n",
    "                m_lang, m_conf = MarathiDetector.detect(text)\n",
    "                if m_lang and m_conf >= 0.35:\n",
    "                    return {\"primary_lang\": m_lang, \"confidence\": round(m_conf, 3), \"method\": \"marathi_markers\"}\n",
    "                \n",
    "                if conf < 0.70 or len(text.split()) < 25:\n",
    "                    return self.llmdetectlanguage(text)\n",
    "            \n",
    "            return {\"primary_lang\": lang, \"confidence\": round(conf, 3), \"method\": \"unicode\"}\n",
    "        \n",
    "        lang, conf = RomanizedHindiDetector.detect(text)\n",
    "        if lang and conf >= 0.25:\n",
    "            return {\"primary_lang\": lang, \"confidence\": round(conf, 3), \"method\": \"romanized\"}\n",
    "        \n",
    "        try:\n",
    "            detected = detect_langs(text)[0]\n",
    "            lang_name = detected.lang.upper()\n",
    "            if lang_name == \"HI\":\n",
    "                lang_name = \"Hindi\"\n",
    "            elif lang_name == \"MR\":\n",
    "                lang_name = \"Marathi\"\n",
    "            elif lang_name == \"EN\":\n",
    "                lang_name = \"English\"\n",
    "            elif lang_name == \"PA\":\n",
    "                lang_name = \"Punjabi\"\n",
    "            return {\"primary_lang\": lang_name, \"confidence\": round(detected.prob, 3), \"method\": \"langdetect\"}\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return self.llmdetectlanguage(text)\n",
    "    \n",
    "    def translate(self, text, lang):\n",
    "        if \"english\" in lang.lower():\n",
    "            return text, 1.0\n",
    "        \n",
    "        prompt = f\"\"\"Translate to fluent English. Preserve proper nouns.\n",
    "Provide a confidence score 0.0-1.0 based on translation accuracy: \n",
    "0.9+ for very clear and natural, 0.6-0.8 for mostly accurate, <0.6 if ambiguous or difficult phrasing.\n",
    "Return ONLY: {{\"translated_text\": \"...\", \"confidence\": 0.XX}}\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "        \n",
    "        # Reduced max_tokens\n",
    "        resp = self.callllm(prompt, max_tokens=200)\n",
    "        parsed = self.parsejson(resp)\n",
    "        \n",
    "        if parsed and \"translated_text\" in parsed:\n",
    "            translated = str(parsed[\"translated_text\"])\n",
    "            conf = float(parsed.get(\"confidence\", 0.88))\n",
    "            if conf == 0.0:\n",
    "                conf = 0.75\n",
    "        else:\n",
    "            translated = resp.strip().strip('\"').strip(\"'\")\n",
    "            conf = 0.7\n",
    "        \n",
    "        return translated, conf\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        prompt = f\"\"\"Analyze this text and return ONLY valid JSON (no markdown, no extra text):\n",
    "\n",
    "{{\n",
    "  \"domain_ident\": [],\n",
    "  \"domain_confidence\": 0.0,\n",
    "  \"sentiment\": \"\",\n",
    "  \"sentiment_confidence\": 0.0,\n",
    "  \"NER\": {{\"Person\": [], \"Location\": [], \"Organisation\": [], \"Event\": [], \"Product\": []}},\n",
    "  \"ner_confidence\": 0.0,\n",
    "  \"Event_calendar\": [],\n",
    "  \"event_calendar_confidence\": 0.0,\n",
    "  \"Country_iden\": \"\",\n",
    "  \"country_confidence\": 0.0,\n",
    "  \"Fact_checker\": {{\"relevant_topics\": [], \"confidence_level\": 0.0, \"relevance_rating\": \"\"}},\n",
    "  \"Summary\": \"\",\n",
    "  \"summary_confidence\": 0.0\n",
    "}}\n",
    "\n",
    "RULES:\n",
    "- domain_ident: ONLY from [Politics, Crime, Military, Terrorism, Radicalisation, Extremism in J&K, Law and Order, Narcotics, Left Wing Extremism, General]\n",
    "- Country_iden: Single country only\n",
    "- Event_calendar: dd/mm/yyyy format, only explicit dates\n",
    "- sentiment: Positive/Negative/Neutral/Anti-National (Anti-National only for direct threats to India)\n",
    "- Summary: One continuous sentence, no newlines. Make it a concise, high-level overview in your own words — do NOT copy or closely paraphrase the input text or translation. Focus on the core event, actors, and implication.\n",
    "- Summary must differ meaningfully from any direct English rendering of the original text — rephrase using different structure and vocabulary.\n",
    "- All confidence scores: realistic values 0.0-1.0\n",
    "- NER: Extract only proper named entities. Person: specific individuals (e.g., \"Eknath Shinde\"), not groups like \"farmers\". Location: specific places (e.g., \"Maharashtra\"). Organisation: specific entities (e.g., \"Maharashtra Legislative Assembly\"). Event: specific named events (e.g., \"2026 winter session\"). Product: specific named products (leave empty if none).\n",
    "\n",
    "Text: {text}\"\"\"\n",
    "        \n",
    "        # Tighter max_tokens cap\n",
    "        max_tokens = min(300, 500 + len(text.split()) // 5)\n",
    "        resp = self.callllm(prompt, max_tokens=max_tokens)\n",
    "        result = self.parsejson(resp)\n",
    "        \n",
    "        if not result:\n",
    "            result = {\n",
    "                \"domain_ident\": [\"General\"], \"domain_confidence\": 0.3,\n",
    "                \"sentiment\": \"Neutral\", \"sentiment_confidence\": 0.3,\n",
    "                \"NER\": {\"Person\": [], \"Location\": [], \"Organisation\": [], \"Event\": [], \"Product\": []},\n",
    "                \"ner_confidence\": 0.0,\n",
    "                \"Event_calendar\": [], \"event_calendar_confidence\": 0.0,\n",
    "                \"Country_iden\": \"Unknown\", \"country_confidence\": 0.0,\n",
    "                \"Fact_checker\": {\"relevant_topics\": [], \"confidence_level\": 0.0, \"relevance_rating\": \"Low\"},\n",
    "                \"Summary\": \"Analysis failed.\", \"summary_confidence\": 0.0\n",
    "            }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def tostr(val):\n",
    "        if isinstance(val, list):\n",
    "            return \", \".join(str(v) for v in val if v)\n",
    "        return str(val) if val else \"\"\n",
    "    \n",
    "    def process(self, text):\n",
    "        cleaned = self.clean(text)\n",
    "        \n",
    "        print(\"Detecting language...\")\n",
    "        lang_info = self.detectlanguage(cleaned)\n",
    "        \n",
    "        # Skip translation if confident English\n",
    "        if \"english\" in lang_info[\"primary_lang\"].lower() and lang_info[\"confidence\"] >= 0.88:\n",
    "            print(\"Skipping translation (confident English detected)...\")\n",
    "            translated = cleaned\n",
    "            trans_conf = 1.0\n",
    "        else:\n",
    "            print(\"Translating...\")\n",
    "            translated, trans_conf = self.translate(cleaned, lang_info[\"primary_lang\"])\n",
    "        \n",
    "        print(\"Analyzing...\")\n",
    "        analysis = self.analyze(translated)\n",
    "        \n",
    "        ner = analysis.get(\"NER\", {})\n",
    "        ner_formatted = {\n",
    "            \"Person\": self.tostr(ner.get(\"Person\", [])),\n",
    "            \"Location\": self.tostr(ner.get(\"Location\", [])),\n",
    "            \"Organisation\": self.tostr(ner.get(\"Organisation\", [])),\n",
    "            \"Event\": self.tostr(ner.get(\"Event\", [])),\n",
    "            \"Product\": self.tostr(ner.get(\"Product\", []))\n",
    "        }\n",
    "        \n",
    "        fc = analysis.get(\"Fact_checker\", {})\n",
    "        \n",
    "        return {\n",
    "            \"Cleaned_content\": cleaned,\n",
    "            \"domain_ident\": self.tostr(analysis.get(\"domain_ident\", [])),\n",
    "            \"domain_ident_confidence_score\": round(float(analysis.get(\"domain_confidence\", 0.0)), 3),\n",
    "            \"lang_det\": lang_info[\"primary_lang\"],\n",
    "            \"lang_det_confidence_score\": lang_info[\"confidence\"],\n",
    "            \"sentiment\": analysis.get(\"sentiment\", \"Neutral\"),\n",
    "            \"sentiment_confidence_score\": round(float(analysis.get(\"sentiment_confidence\", 0.0)), 3),\n",
    "            \"NER\": ner_formatted,\n",
    "            \"NER_confidence_score\": round(float(analysis.get(\"ner_confidence\", 0.0)), 3),\n",
    "            \"Event_calender\": self.tostr(analysis.get(\"Event_calendar\", [])),\n",
    "            \"Event_calender_confidence_score\": round(float(analysis.get(\"event_calendar_confidence\", 0.0)), 3),\n",
    "            \"Country_iden\": analysis.get(\"Country_iden\", \"Unknown\"),\n",
    "            \"Country_iden_confidence_score\": round(float(analysis.get(\"country_confidence\", 0.0)), 3),\n",
    "            \"Summary\": str(analysis.get(\"Summary\", \"\")),\n",
    "            \"Summary_confidence_score\": round(float(analysis.get(\"summary_confidence\", 0.0)), 3),\n",
    "            \"Fact_checker\": self.tostr(fc.get(\"relevant_topics\", [])),\n",
    "            \"Fact_checker_relevance_rating\": fc.get(\"relevance_rating\", \"Low\"),\n",
    "            \"Fact_checker_confidence_score\": round(float(fc.get(\"confidence_level\", 0.0)), 3),\n",
    "            \"Translation\": translated,\n",
    "            \"Translation_confidence_score\": round(trans_conf, 3)\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    content = '''महाराष्ट्र विधानसभेच्या २०२६ च्या हिवाळी अधिवेशनात शेतकरी कर्जमाफी आणि बेरोजगारीवरून खडाजंगी झाली. मुख्यमंत्री एकनाथ शिंदे यांनी आर्थिक स्थितीवर श्वेतपत्रिका काढण्याचे आश्वासन दिले आहे.'''\n",
    "    \n",
    "    nlp = NLPOrchestrator()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"NLP ORCHESTRATION PIPELINE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    result = nlp.process(content)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FINAL OUTPUT\")\n",
    "    print(\"=\" * 60)\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
